{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U ucimlrepo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "The helper modules for data loading and exploration are already imported. The **data_loader** module loads the \"auto_mpg\" dataset, and **data_explorer** extracts and prints the dataset's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_loader as dl\n",
    "from utils import data_explorer as de\n",
    "import numpy as np\n",
    "\n",
    "data_loader = dl.DataLoader()\n",
    "auto_mpg_data = data_loader.get_dataset(\"auto_mpg\")\n",
    "data_explorer = de.DataExplorer(auto_mpg_data)\n",
    "metadata = data_explorer.describe_data()\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1**: Check for Missing Data\n",
    "\n",
    "Check if there are any missing values in the dataset and removing rows that contain missing data.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Check for columns with missing values.\n",
    "- Remove the rows with missing values.\n",
    "- Print out the number of rows before and after cleaning. Expected answer: 398 and 392\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_missing_data = auto_mpg_data.isna().sum()\n",
    "missing_columns = ch_missing_data[ch_missing_data > 0].index  \n",
    "rows_with_missing_data = auto_mpg_data[auto_mpg_data[missing_columns].isna().any(axis=1)]\n",
    "print(f\"Features with missing data: {missing_columns[0]}\")\n",
    "print(f\"rows with missing data {rows_with_missing_data.shape[0]}\")\n",
    "df = auto_mpg_data.dropna()\n",
    "print(f\"Number of samples Raw dataset: {auto_mpg_data.shape[0]}\")\n",
    "print(f\"Number of samples cleaned dataset: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2**: Define your target variable (mpg) and features (all other numerical columns), and then split the data into training and testing sets.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "- Define the target (mpg) and select the numerical columns as features.\n",
    "- Split the data into training and testing sets (80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'mpg'\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "features = numerical_cols[numerical_cols != target]\n",
    "\n",
    "X = df[features].values\n",
    "y = df[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3**: Preprocess the Data (Scaling)\n",
    "Preprocess the data by normalizing the features using StandardScaler. This is an important step to ensure that the features are on the same scale.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "- Normalize the training and testing features using the StandardScaler.\n",
    "- Convert the data to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-processing step. \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using the training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Shape should be (n_samples, 1)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4**:  Build the Neural Network Model\n",
    "Define and build the neural network model. For this task, you will use `PyTorch` to define a simple feedforward neural network with one hidden layer. The model will take the normalized features as input and output the predicted fuel efficiency (MPG).\n",
    "\n",
    "\n",
    "**Task**:\n",
    "\n",
    "- Define a neural network model using nn.Sequential() or by creating a custom class that inherits from nn.Module.\n",
    "- The model should include:\n",
    "    - An input layer that matches the number of features.\n",
    "    - At least one hidden layer of 64 neuros and a ReLU activation function.\n",
    "    - An output layer with one neuron (since the target is a single continuous value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "hidden_sizes = [64]\n",
    "output_size = 10\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])  \n",
    "        self.relu = nn.ReLU()                 \n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], 1)           \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)      \n",
    "        x = self.relu(x)     \n",
    "        x = self.fc2(x)      \n",
    "        return x\n",
    "\n",
    "\n",
    "input_size = X_train.shape[1]  \n",
    "model = SimpleNN(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5**:  Train the Neural Network Model\n",
    "Train the neural network model. This involves defining the loss function, specifying the optimizer, and running the training loop. The goal is to minimize the loss (difference between the predicted and actual MPG values) using gradient descent.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "- Define a loss function (e.g., Mean Squared Error for regression tasks).\n",
    "- Choose an optimizer (e.g., Adam, which is commonly used for neural network training).\n",
    "- Write the training loop where the model learns from the training data, calculates the loss, and updates the weights through backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()          \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "loss_values_train = []\n",
    "loss_values_test = []\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()  \n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_tensor)  \n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_values_train.append(loss.item())\n",
    "    \n",
    "    # Testing phase (no gradient computation)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = model(X_test_tensor)\n",
    "        loss_test = criterion(y_pred_test, y_test_tensor)\n",
    "        loss_values_test.append(loss_test.item())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(epochs), loss_values_train, label=\"Training Loss\", color=\"blue\")\n",
    "plt.plot(range(epochs), loss_values_test, label=\"Testing Loss\", color=\"orange\")\n",
    "plt.title(\"Loss Curve Over Training Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6**:  Evaluate the Model's Performance\n",
    "Evaluate how well the model performs on the train and test dataset that it has not seen before. This helps determine if the model generalizes well to new data or if it overfits to the training data.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "- Use the trained model to make predictions on the train/test data.\n",
    "- Calculate evaluation metrics :Mean Squared Error (MSE).\n",
    "- Visualize the results using plots like predicted vs. actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Make predictions on the test set, Turn off gradient\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_tensor)\n",
    "\n",
    "# Compute the Mean Squared Error\n",
    "test_loss = criterion(y_pred_test, y_test_tensor)\n",
    "print(f\"Test Loss (MSE): {test_loss.item():.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor)\n",
    "\n",
    "# Compute the Mean Squared Error\n",
    "train_loss = criterion(y_pred_train, y_train_tensor)\n",
    "print(f\"Train Loss (MSE): {train_loss.item():.4f}\")\n",
    "\n",
    "y_pred_test  = y_pred_test.numpy()\n",
    "y_pred_train = y_pred_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df_plot_train = pd.DataFrame({\n",
    "    'Actual_train': y_train,\n",
    "    'Predicted_train': y_pred_train.flatten()\n",
    "})\n",
    "min_train = df_plot_train['Actual_train'].min()\n",
    "max_train = df_plot_train['Actual_train'].max()\n",
    "\n",
    "df_plot_test = pd.DataFrame({\n",
    "    'Actual_test': y_test,\n",
    "    'Predicted_test': y_pred_test.flatten()\n",
    "})\n",
    "\n",
    "min_test = df_plot_test['Actual_test'].min()\n",
    "max_test = df_plot_test['Actual_test'].max()\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "sns.scatterplot(data=df_plot_train, x='Actual_train', y='Predicted_train', color = 'blue', marker = 'o', label = 'Training data')\n",
    "sns.scatterplot(data=df_plot_test, x='Actual_test', y='Predicted_test', color = 'green', marker = '^', label = 'Testing data')\n",
    "plt.plot([min(min_train, min_test), max(max_train, max_test)], [min(min_train, min_test), max(max_train, max_test)], color='red', linestyle='--', label='y_pred = y_actual') \n",
    "plt.title('Actual vs Predicted values after Optimization')\n",
    "plt.xlabel('Actual values (y)')\n",
    "plt.ylabel('Predicted values (y_pred)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7**: Save the Model\n",
    "\n",
    "**Task:**\n",
    "- Save the trained model using PyTorch's `torch.save()` function.\n",
    "- Ensure the model state dictionary (**model.state_dict()**) is saved, as it contains the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'input_size': input_size,  \n",
    "    'output_size': output_size,  \n",
    "    'hidden_layers': hidden_sizes,  \n",
    "    'state_dict': model.state_dict()  \n",
    "}\n",
    "\n",
    "model_path = f\"./models/{target}_predictor_model.pth\"\n",
    "torch.save(checkpoint, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
