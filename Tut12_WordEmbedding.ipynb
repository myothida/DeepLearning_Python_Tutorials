{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tutorial 10: Word Embeddings**\n",
    "\n",
    "In this tutorial, we will explore the concept of **Word Embeddings**, a technique to represent words as dense vectors in a continuous vector space. Word embeddings capture semantic meanings and relationships between words, enabling machines to understand and process natural language more effectively. These embeddings are a key component in various Natural Language Processing (NLP) applications, such as sentiment analysis, machine translation, and text generation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. What Are Word Embeddings?**\n",
    "Word embeddings are numerical vector representations of words that encode their semantic meaning. Unlike traditional one-hot encoding, which is sparse and lacks contextual meaning, word embeddings are dense and contain meaningful relationships between words.\n",
    "\n",
    "- **One-Hot Encoding**:  \n",
    "  A binary vector where each word is represented as a unique index. For example:  \n",
    "  `[\"apple\", \"orange\", \"banana\"] → [1, 0, 0] (for \"apple\")`  \n",
    "  - **Problems**: High dimensionality, no semantic similarity captured.\n",
    "\n",
    "- **Word Embeddings**:  \n",
    "  Dense vectors that encode semantic meaning and relationships between words. For example:  \n",
    "  `[\"apple\"] → [0.9, 0.2, -0.1]`  \n",
    "  - **Advantages**: Low dimensionality, meaningful relationships captured.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Why Word Embeddings?**\n",
    "1. **Semantic Relationships**: Words with similar meanings are closer in vector space.\n",
    "   - Example: `\"king\" - \"man\" + \"woman\" ≈ \"queen\"`\n",
    "2. **Low Dimensionality**: Dense vectors reduce memory usage and improve computational efficiency.\n",
    "3. **Context-Aware**: Advanced embeddings like Word2Vec, GloVe, and BERT capture contextual meaning.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Common Techniques to Generate Word Embeddings**\n",
    "\n",
    "##### **a. Word2Vec**\n",
    "A neural network-based method introduced by Google that learns word embeddings using:\n",
    "- **CBOW (Continuous Bag of Words)**: Predict a word based on its context.\n",
    "- **Skip-gram**: Predict context words given a word.\n",
    "\n",
    "##### **b. GloVe (Global Vectors for Word Representation)**\n",
    "A method that combines global word co-occurrence statistics to generate embeddings.\n",
    "\n",
    "##### **c. FastText**\n",
    "An extension of Word2Vec that represents words as n-grams, capturing subword information and handling rare words effectively.\n",
    "\n",
    "##### **d. Pre-trained Models**\n",
    "Modern embeddings like BERT, GPT, and embeddings derived from transformer models capture word context dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Applications of Word Embeddings**\n",
    "- **Sentiment Analysis**: Use embeddings to represent text for classifying sentiments.\n",
    "- **Machine Translation**: Translate text by mapping embeddings between languages.\n",
    "- **Recommendation Systems**: Use embeddings to find similar items based on descriptions.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Challenges**\n",
    "- **Out-of-Vocabulary Words**: Rare or unseen words may not have embeddings.\n",
    "- **Context Independence**: Basic embeddings like Word2Vec don't account for word sense disambiguation.\n",
    "- **Memory Usage**: Large embeddings can consume significant memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **4. Implementation of Word2Vec**\n",
    "## Word2Vec using the **Gensim** library in Python.\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus = [\n",
    "    [\"yangon\", \"is\", \"a\", \"city\", \"with\", \"a\", \"rich\", \"history\"],\n",
    "    [\"bagan\", \"is\", \"famous\", \"for\", \"its\", \"ancient\", \"temples\", \"and\", \"pagodas\"],\n",
    "    [\"the\", \"irrawaddy\", \"river\", \"is\", \"the\", \"lifeline\", \"of\", \"myanmar\"],\n",
    "    [\"traditional\", \"foods\", \"like\", \"mohinga\", \"are\", \"popular\", \"in\", \"myanmar\"],\n",
    "    [\"the\", \"shwedagon\", \"pagoda\", \"is\", \"a\", \"sacred\", \"site\", \"in\", \"yangon\"],\n",
    "    [\"bamboo\", \"houses\", \"are\", \"common\", \"in\", \"rural\", \"areas\", \"of\", \"myanmar\"],\n",
    "    [\"mandalay\", \"is\", \"known\", \"for\", \"its\", \"cultural\", \"heritage\"],\n",
    "    [\"inle\", \"lake\", \"is\", \"famous\", \"for\", \"floating\", \"gardens\", \"and\", \"leg\", \"rowers\"],\n",
    "    [\"kuthodaw\", \"pagoda\", \"is\", \"home\", \"to\", \"the\", \"world's\", \"largest\", \"book\"],\n",
    "    [\"the\", \"thanaka\", \"paste\", \"is\", \"used\", \"as\", \"a\", \"traditional\", \"cosmetic\", \"in\", \"myanmar\"]\n",
    "]\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, workers=4)\n",
    "\n",
    "vector = model.wv['city']\n",
    "print(\"Vector for 'city':\", vector)\n",
    "\n",
    "similar_words = model.wv.most_similar('city', topn=3)\n",
    "print(\"Words similar to 'city':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "words = list(model.wv.index_to_key)\n",
    "vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word, vector in zip(words, reduced_vectors):\n",
    "    plt.scatter(vector[0], vector[1], alpha=0.7)\n",
    "    plt.annotate(word, (vector[0], vector[1]), fontsize=10, alpha=0.8)\n",
    "\n",
    "plt.title(\"Word Embeddings Visualization\")\n",
    "plt.xlabel(\"Reduced Dimension 1\")\n",
    "plt.ylabel(\"Reduced Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`torch.nn.Embedding`**\n",
    "\n",
    "The `nn.Embedding` layer in PyTorch is used to map discrete integers (like word indices) to dense vectors of fixed size. It acts as a lookup table that stores embeddings (dense representations) for each index in the vocabulary.\n",
    "\n",
    "- **Random Initialization**: The `nn.Embedding` layer starts with a randomly initialized embedding matrix, meaning the vectors assigned to tokens initially have no semantic meaning.\n",
    "\n",
    "- **Trainable Weights**: During training, the embedding matrix is updated via backpropagation to learn meaningful representations of the tokens in the context of the specific task (e.g., text classification, language modeling).\n",
    "\n",
    "- **Token-Specific Lookups**: The embedding layer retrieves the rows corresponding to the token indices, ensuring that each token gets its specific dense representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.tokenizer import CharTokenizer, WordTokenizer\n",
    "\n",
    "text_paragraph = \"\"\"\n",
    "You are a manufacturer of hip implants. The doctor who will use your implants in surgeries has a requirement: he is willing to accept implants that are 1 mm bigger or smaller than the specified target size. This means the implant sizes must fall within a 2 mm range of the target size, i.e., ±1 mm from the target.\n",
    "Additionally, your financial officer has stated that in order to maintain profitability, you can afford to discard **1 out of every 1000 implants**. This means that the size distribution of your implants must be such that only 0.1% of implants fall outside the acceptable ±1 mm range.\n",
    "Given a recent sample of 1000 implants from the factory, the task is to evaluate whether the factory is meeting the specified quality and profitability requirements. If more than one percent of the implants fall outside the ±1 mm range, the factory will incur a loss due to excess waste.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = WordTokenizer.train_from_text(text_paragraph)\n",
    "vocab_size = tokenizer.vocabulary_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "text_input = \"Given a recent sample of \"\n",
    "input_tokens = tokenizer.encode(text_input)\n",
    "print(f\"Ecoded Vector of 'Given a recent sample of ' is: {input_tokens}\")\n",
    "\n",
    "\n",
    "embedding_dim = 4\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "output_embeddings = embedding_layer(input_tokens)\n",
    "\n",
    "print(f\"Text input: {text_input}\")\n",
    "print(\"Input tokens:\", input_tokens)\n",
    "print(\"Embedded vector:\", output_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanation of nn.Embedding Layer**\n",
    "\n",
    "#### 1. Vocabulary Encoding\n",
    "The vocabulary size is 96. Each word or token in the input text is assigned a unique index (integer ID) in the range [0, 95].\n",
    "\n",
    "For the input phrase \"Given a recent sample of\", the tokenizer maps the words to their indices as follows:\n",
    "\n",
    "- \"Given\" -> 6  \n",
    "- \"a\" -> 11  \n",
    "- \"recent\" -> 66  \n",
    "- \"sample\" -> 69  \n",
    "- \"of\" -> 51\n",
    "\n",
    "#### 2. Input to the Embedding Layer\n",
    "The indices [6, 11, 66, 69, 51] are fed into the `nn.Embedding` layer. The layer contains a trainable embedding matrix of shape (vocabulary_size, embedding_dim):\n",
    "\n",
    "- 96 rows: One for each token in the vocabulary.\n",
    "- 4 columns: Representing the `embedding_dim` (size of each token's vector).\n",
    "\n",
    "#### 3. Lookup Operation\n",
    "When the input indices [6, 11, 66, 69, 51] are passed to the embedding layer, it performs a lookup in the embedding matrix. Specifically:\n",
    "\n",
    "- Index 6 retrieves the 6th row of the matrix.\n",
    "- Index 11 retrieves the 11th row, and so on.\n",
    "\n",
    "Example lookups:\n",
    "\n",
    "- `embedding_matrix[6]` -> `[-0.7583, 0.9922, 1.8787, 3.0013]`\n",
    "- `embedding_matrix[11]` -> `[-0.0526, 1.0764, -0.5330, -1.4174]`\n",
    "\n",
    "#### 4. Embedded Vector Output\n",
    "The output is a tensor of shape is `(number of tokens, embedding_dim)`\n",
    "```\n",
    "tensor([[-0.7583,  0.9922,  1.8787,  3.0013],  # \"Given\"\n",
    "        [-0.0526,  1.0764, -0.5330, -1.4174],  # \"a\"\n",
    "        [ 0.2218, -1.2806, -0.2778, -0.3278],  # \"recent\"\n",
    "        [ 0.2442, -1.1955,  0.5330, -0.7915],  # \"sample\"\n",
    "        [ 0.8490,  1.0200, -1.4119, -1.9483]]) # \"of\"\n",
    "\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embedding Explanation\n",
    "\n",
    "In transformer models, positional embeddings are used to encode the position of each token in the input sequence. Since transformers process the entire input sequence at once (in parallel), they don't inherently capture the order of tokens, unlike RNNs or LSTMs, which process tokens sequentially. To overcome this, we add positional embeddings to the input token embeddings, allowing the model to learn and understand the order of tokens in the sequence.\n",
    "\n",
    "**Positional embeddings** are trainable parameters, just like token embeddings, and they are updated during training through backpropagation. Their primary role is to help the model understand the relative or absolute positions of tokens in a sequence, which is essential for sequence-based tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "token_embeddings = embedding_layer(input_tokens)\n",
    "\n",
    "\n",
    "context_size = len(input_tokens) \n",
    "positional_embedding_layer = nn.Embedding(context_size, embedding_dim)\n",
    "positions = torch.arange(context_size).long()  \n",
    "positional_embeddings = positional_embedding_layer(positions)\n",
    "\n",
    "\n",
    "# Add token embeddings and positional embeddings to get the final embeddings\n",
    "final_embeddings = token_embeddings + positional_embeddings\n",
    "\n",
    "print(f\"Text input: {text_input}\")\n",
    "print(\"Input tokens:\", input_tokens)\n",
    "print(\"Token embeddings:\\n\", token_embeddings)\n",
    "print(\"Positional embeddings:\\n\", positional_embeddings)\n",
    "print(\"Final embeddings (Token + Positional):\\n\", final_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
