{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial 1: Understanding Tensors and Basic Math with Linear Algebra  \n",
    "\n",
    "In this tutorial, we will explore tensors, the building blocks of deep learning, and understand their role in computations. We'll also cover some basic math and linear algebra exercises to strengthen your foundation.  \n",
    "\n",
    "##### What is a Tensor?\n",
    "A tensor is a multi-dimensional array, a generalization of scalars, vectors, and matrices. Tensors allow us to represent data in various dimensions:  \n",
    "\n",
    "- **0D Tensor (Scalar):** A single value.  \n",
    "  Example: `3` or `-7.5`  \n",
    "\n",
    "- **1D Tensor (Vector):** A collection of numbers in a single dimension.  \n",
    "  Example: `[1, 2, 3]`  \n",
    "\n",
    "- **2D Tensor (Matrix):** Numbers arranged in rows and columns.  \n",
    "  \n",
    "- **3D Tensor and Higher Dimension:** A color image (height √ó width √ó RGB channels).  \n",
    "\n",
    "<img src=\"imgs/tensor_examples.svg\" width=600px>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Exercises  \n",
    "##### Task 01: Create Tensors \n",
    "Use PyTorch to create tensors for the following variables. Ensure that each tensor is initialized with the correct shape and data.\n",
    "\n",
    "- **Features**: Create a tensor with shape (1, 5) (one row and five columns) using random values from a normal distribution.\n",
    "- **Weights**: Create a tensor with shape (1, 5) (one row and five columns), also initialized with random values from a normal distribution.\n",
    "- **Bias**: Create a constant scalar tensor (shape (1, 1)) initialized with a random value from a normal distribution.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "### set the random seed to 7\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 02: Compute the Values of $ùë¶$ \n",
    "\n",
    "Given the equation, $ y(x, \\omega) = \\omega_0 + \\omega_1x_1 + \\omega_2x_2 + \\omega_3x_3 $\n",
    "Where:  \n",
    "- $x$ represents the features (input data). \n",
    "- $\\omega_i$ represents the weights corresponding to each feature.\n",
    "- $\\omega_0$ represents the bias (a constant added to the output).  \n",
    "\n",
    "### Instructions:  \n",
    "1. Perform **element-wise multiplication** between the `features` tensor and the `weights` tensor.  \n",
    "2. **Sum** the results of the element-wise multiplication.  \n",
    "3. **Add the bias** term to the summed result to compute the final value of $ y $. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 03: Reshape and Compute\n",
    "Modify the features tensor to represent a batch of 3 samples. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 04: Element-wise Multiplication and Summation\n",
    "\n",
    "Perform element-wise multiplication of `features` and `weights` for the batch of samples created in Task 03. Sum the results row-wise and add the bias to each row. Verify that this matches the result of directly using PyTorch‚Äôs [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 05: Apply the Sigmoid Function  \n",
    "\n",
    "The **sigmoid function** is commonly used in neural networks to introduce non-linearity. It is defined as:  \n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where $z$ is the input value.  \n",
    "\n",
    "### Instructions:  \n",
    "1. Create a sigmoid activation function.\n",
    "2. Using the $y$ value computed in **Task 02** as the input $ z $ for the sigmoid function, Compute the sigmoid of $y$ to get the final output. Expected Output: `0.1595`\n",
    "3. Using the $y$ value computed in **Task 04** as the input $ z $ for the sigmoid function, Compute the sigmoid of $y$ to get the final output. Expected Output: `[0.1643,0.1639,0.9053]`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 06: Apply the Softmax Function  \n",
    "\n",
    "The **Softmax function** is commonly used in neural networks for multi-class classification tasks. It converts a vector of raw scores into probabilities, where the probabilities sum to 1. The softmax function is defined as:  \n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "where $z_i$ is the $i$-th input value, and the denominator is the sum of the exponentials of all input values in the vector.\n",
    "\n",
    "### Instructions:  \n",
    "1. Create a softmax activation function.\n",
    "2. Using the $y$ value computed in **Task 02** as the input $ z $ for the softmax function, Compute the softmax of $y$ to get the final output. \n",
    "3. Using the $y$ value computed in **Task 04** as the input $ z $ for the softmax function, Compute the softmax of $y$ to get the final output. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
