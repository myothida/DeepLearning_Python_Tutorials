{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tutorial 06: Using Pre-Trained Models**\n",
    "\n",
    "**Pre-trained Models (networks):** are models that have been previously trained on large benchmark datasets like ImageNet, and are now available for reuse. The key advantage of pre-trained models is that they have already learned to extract general features from images, such as edges, textures, and more complex patterns. This allows them to be fine-tuned for your specific task with much less data and computational cost than training a model from scratch.\n",
    "\n",
    "\n",
    "##### **Common Pre-trained Models in TorchVision**\n",
    "\n",
    "TorchVision offers a wide range of pre-trained models that have been trained on large benchmark datasets like ImageNet. These models can be used directly for inference or fine-tuned for specific tasks.\n",
    "\n",
    "**ResNet (Residual Networks)**: \n",
    "- ResNet models (ResNet18, ResNet34, ResNet50, ResNet101, etc.) are some of the most popular architectures for image classification tasks. These models use residual blocks to mitigate the vanishing gradient problem in deep networks.\n",
    "- Example: torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "**VGG**\n",
    "- VGG networks (VGG11, VGG16, VGG19) are known for their simplicity and deep architecture. These networks consist of stacked convolutional layers followed by fully connected layers.\n",
    "- Example: torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "**DenseNet**\n",
    "- DenseNet (DenseNet121, DenseNet169) connects each layer to every other layer in a feed-forward fashion, improving gradient flow and feature reuse.\n",
    "- Example: torchvision.models.densenet121(pretrained=True)\n",
    "\n",
    "**AlexNet**\n",
    "- One of the first deep neural networks for image classification, AlexNet consists of multiple convolutional layers followed by fully connected layers.\n",
    "- Example: torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "**InceptionV3**\n",
    "\n",
    "- InceptionV3 is a deep neural network designed to optimize computational efficiency while maintaining high performance. It uses inception blocks that apply multiple filters of different sizes simultaneously.\n",
    "- Example: torchvision.models.inception_v3(pretrained=True)\n",
    "\n",
    "**MobileNetV2**\n",
    "- MobileNetV2 is a lightweight model optimized for mobile and edge devices. It uses depthwise separable convolutions for computational efficiency.\n",
    "- Example: torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "**EfficientNet**\n",
    "- EfficientNet is a family of models that scale depth, width, and resolution to achieve higher accuracy with fewer parameters.\n",
    "- Example: torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "**Vision Transformer (ViT)**\n",
    "- Vision Transformers are based on the Transformer architecture, which was originally designed for natural language processing tasks. They are becoming increasingly popular in image classification.\n",
    "- Example: torchvision.models.vit_b_16(pretrained=True)\n",
    "\n",
    "### Using the Pre-trained model to test your data\n",
    "\n",
    "To use a pre-trained model for inference or evaluation on your own dataset, you first need to load the model and modify the output layer to suit your task. For example, if you're working with a 10-class classification problem, you'll need to change the final layer to output 10 classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task**: Using Pre-trained Models on Own Images\n",
    "\n",
    "##### **Objective**:\n",
    "Learn how to use and evaluate the performance of pre-trained models (at least 5 different models) on images in the **pet images** folder and compute classification metrics such as True Positives (TP), False Positives (FP), and Accuracy. The results should be reported in a table with the first column as the model name and the rest of the columns as TP, FP, and Accuracy.\n",
    "\n",
    "#### **Pre-trained Models**:\n",
    "Evaluate the following models on the **pet images** dataset. \n",
    "\n",
    "1. **ResNet50**\n",
    "2. **AlexNet**\n",
    "3. **VGG16**\n",
    "4. **DenseNet121**\n",
    "5. **MobileNetV2**\n",
    "\n",
    "These models are all pre-trained on the ImageNet dataset and are available in popular frameworks like PyTorch.\n",
    "\n",
    "#### **Metrics to Compute**: For each model, compute the following:\n",
    "\n",
    "- **True Positives (TP)**: Correctly classified positive samples.\n",
    "- **False Positives (FP)**: Incorrectly classified negative samples as positive.\n",
    "- **Accuracy**: The overall accuracy of the model.\n",
    "\n",
    "#### **Compare the Results**: After evaluating all models, compile the results into a table. The table should have the following structure:\n",
    "\n",
    "| Model          | True Positives (TP) | False Positives (FP) | Accuracy (%) |\n",
    "|----------------|---------------------|----------------------|--------------|\n",
    "| ResNet50       | 500                 | 20                   | 95.0         |\n",
    "| AlexNet        | 450                 | 50                   | 90.0         |\n",
    "| VGG16          | 480                 | 40                   | 92.0         |\n",
    "| DenseNet121    | 470                 | 30                   | 93.5         |\n",
    "| MobileNetV2    | 490                 | 10                   | 96.0         |\n",
    "\n",
    "\n",
    "##### **Code Helper**: Below code is provided to test on a single image using the ResNet50 Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1, Prob: tensor([-0.3211])\n",
      "Predicted label: goldfish\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from utils import data_loader\n",
    "\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "\n",
    "image_path = \"./data/pet_images/cat_01.jpg\"\n",
    "image = cv2.imread(image_path, cv2.IMREAD_COLOR) \n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# Convert to PIL Image\n",
    "image_pil = Image.fromarray(image_rgb)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_tensor = preprocess(image_pil)\n",
    "input_batch = input_tensor.unsqueeze(0)  \n",
    "\n",
    "model.eval()  \n",
    "with torch.no_grad():  # No gradients needed for inference\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get predicted class\n",
    "prob, predicted_class = torch.max(output, 1)\n",
    "print(f\"Predicted class: {predicted_class.item()}, Prob: {prob}\")\n",
    "\n",
    "# ImageNet class labels\n",
    "dl = data_loader.DataLoader()\n",
    "class_idx = predicted_class.item()\n",
    "\n",
    "class_idx_to_label = dl.imagenet1000_cls_id_label()\n",
    "predicted_label = class_idx_to_label[str(class_idx)][1]\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
