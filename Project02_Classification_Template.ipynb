{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1**: Load the Dataset\n",
    "\n",
    "Load the MNIST dataset and inspect the data structure.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Load the dataset using the provided data loader.\n",
    "- Explore the dimensions of the training dataset.\n",
    "---\n",
    "\n",
    "#### **Step 2**: Preprocess the Data\n",
    "Prepare the data for training by normalizing pixel values and converting data into tensors.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Convert the dataset into PyTorch tensors.\n",
    "- Normalize the pixel values to a range of -1 to 1. (mean =0.5, std = 0.5)\n",
    "- Visualize a few sample images along with their labels.\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_loader as dl\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_loader = dl.DataLoader()\n",
    "mnist_trdata, mnist_testdata = data_loader.get_dataset(\"mnist\")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(mnist_trdata, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(mnist_testdata, batch_size=64, shuffle=True)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(type(images))\n",
    "print(f\"Image dimension: {images.shape[1:]}\")\n",
    "print(f\"Number of images in the 1st batch set: {labels.shape[0]}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))  \n",
    "for k in range(3):\n",
    "    axes[k].imshow(images[k].numpy().squeeze(), cmap='Greys_r')  \n",
    "    axes[k].set_title(f\"Image label is {labels[k]}\")\n",
    "    axes[k].axis('off')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3**: Build the Neural Network\n",
    "Define a neural network to classify the digits.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Design a network architecture with \n",
    "    - **Input Layer**: 784 input units (corresponding to flattened 28x28 pixel images).\n",
    "    - **First hidden layer** with 128 units.\n",
    "    - **Second hidden layer** with 64 units.\n",
    "    - **Output Layer**: - 10 units (corresponding to the 10 possible digit classes: 0â€“9).\n",
    "-  Use **ReLU** (Rectified Linear Unit) for the two hidden layers to introduce non-linearity.\n",
    "-  Use **Softmax** for the output layer to produce probabilities for each digit class.\n",
    "- Create the neural network structure based on the above specifications and Initialize the model.\n",
    "- Before training the model, test its forward pass using a single input image to verify that the network is functioning as expected.\n",
    "- Visualize the the output using **helper module**.\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from utils import view_helper\n",
    "\n",
    "# Complete the code here: Define architecture\n",
    "\n",
    "\n",
    "# Complete the code here: Build a feed-forward network\n",
    "model =nn.Sequential()\n",
    "\n",
    "\n",
    "# Test an image before training\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "img = images[img_idx]\n",
    "view_helper.view_classify(img.view(1, 28, 28), ps)\n",
    "print(f\"The highest probablity {ps.topk(1).values.item()* 100:.2f}% at class: {ps.topk(1).indices.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4**: Define Loss Function and Optimizer\n",
    "Set up a suitable loss function and optimizer to train the model.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- **Use Cross-Entropy Loss** as the loss function.\n",
    "    - **Note**: `nn.CrossEntropyLoss()` is commonly used when the model outputs raw logits (i.e., before applying LogSoftmax). It internally applies Softmax and computes the negative log-likelihood loss.\n",
    "    - **`nn.NLLLoss()`** is used when the model outputs log-probabilities (i.e., after applying LogSoftmax). It expects log-probabilities as inputs.\n",
    "\n",
    "- **Select an optimizer**, such as SGD or Adam, and set the learning rate.\n",
    "    - Example:\n",
    "    ```python\n",
    "    criterion = nn.CrossEntropyLoss()  # or nn.NLLLoss() if using LogSoftmax in the model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# define criterion and optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5**: Train the Model\n",
    "Train the neural network on the training dataset over multiple epochs.\n",
    "\n",
    "**Task:**\n",
    "- Loop through the training data for a specified number of epochs.\n",
    "- In each epoch:\n",
    "    - Perform a forward pass to compute the predictions.\n",
    "    - Compute the loss for both training and testing.\n",
    "    - Backpropagate the error and update the model weights.\n",
    "    - Track the training/testing loss for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "# Build the model here. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6**: Visualize Results\n",
    "Visualize the model's performance using appropriate plots.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Plot the training/testing loss curve over epochs.\n",
    "- Display a sample image using **helper module** and print the actual and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    ps = model(img)\n",
    "\n",
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "view_helper.view_classify(img.view(1, 28, 28), ps)\n",
    "print(f\"The highest probablity: {ps.topk(1).values.item()*100:.2f}% at class: {ps.topk(1).indices.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7**: Evaluate the Model\n",
    "Assess the model's performance on the testing dataset.\n",
    "**Task:**\n",
    "- Perform predictions on the test dataset.\n",
    "- Generate a confusion matrix to analyze classification errors.\n",
    "- Calculate accuracy by comparing predicted labels with actual labels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "pred_class = []\n",
    "actual_class = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:   \n",
    "        images = images.view(images.shape[0], -1)         \n",
    "        ps = model(images)\n",
    "        ## Complete the code here:\n",
    "        pass\n",
    "        \n",
    "\n",
    "pred_class = np.array(pred_class).flatten() \n",
    "actual_class = np.array(actual_class).flatten()               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "class_labels = pd.DataFrame({'Predicted': pred_class, 'Truth_Label': actual_class})\n",
    "class_counts = class_labels['Truth_Label'].value_counts().sort_index()\n",
    "cm = confusion_matrix(actual_class, pred_class)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "TP = np.diag(cm)  \n",
    "FP = np.sum(cm, axis=0) - TP  \n",
    "FN = np.sum(cm, axis=1) - TP  \n",
    "\n",
    "result_df = pd.DataFrame({\"True Positive\": TP, \"False Positive\": FP, \"False Negative\": FN, \"Number_Samples\": class_counts.values})\n",
    "print(f\"Accuracy of the model is: {result_df['True Positive'].sum()/len(class_labels):0.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 8**: Save the Model\n",
    "\n",
    "**Task:**\n",
    "- Save the trained model using PyTorch's `torch.save()` function.\n",
    "- Ensure the model state dictionary (**model.state_dict()**) is saved, as it contains the model parameters.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
