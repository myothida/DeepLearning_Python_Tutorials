{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project: Transfer Learning**\n",
    "\n",
    "**Transfer learning** leverages the pre-trained models by taking advantage of the learned features from one task and applying them to another, reducing the time and effort required for model training and enhancing performance, especially with smaller datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transfer Learning: Fine-Tuning Parameters**\n",
    "Transfer learning is a powerful technique in deep learning where you take a pre-trained model and adapt it for a new task. The pre-trained model is usually trained on a large benchmark dataset (like ImageNet), which allows it to learn general features that can be useful for a wide range of tasks. Fine-tuning is one of the approaches to leveraging pre-trained models for a new problem.\n",
    "\n",
    "**Understanding Fine-Tuning**\n",
    "\n",
    "In a typical pre-trained model, there are two main parts:\n",
    "- **Feature extraction layers**: These are usually convolutional layers that are used to detect important features like edges, textures, and patterns in images. These features are often general and transferable across different tasks.\n",
    "- **Classifier layers**: These are usually fully connected layers that take the extracted features and make predictions based on them. These layers are specific to the dataset on which the model was trained (e.g., ImageNet).\n",
    "\n",
    "Fine-tuning involves making adjustments to the entire pre-trained model or part of it to adapt it to a new dataset, rather than starting from scratch. \n",
    "\n",
    "\n",
    "#### Steps for Fine-Tuning a Pre-Trained Model\n",
    "\n",
    "1. **Load a Pre-Trained Model**: In PyTorch, pre-trained models from TorchVision are readily available. You can use models like ResNet, VGG, and others that have been trained on large datasets like ImageNet.\n",
    "2. **Replace the Classifier Layers**: Since the original model is trained on a different dataset (e.g., ImageNet), we need to replace its classifier layers to match the number of classes in our new dataset.\n",
    "3. **Freeze the Pre-Trained Layers (Optional)**: In fine-tuning, you can choose to freeze the weights of the pre-trained (feature extraction) layers and only update the classifier layer. Freezing layers means preventing their weights from being updated during training, which can help when you have limited data.\n",
    "4. **Define the Loss Function and Optimizer**: Now, we need to set up the loss function and optimizer for training. The loss function depends on the type of task you are performing (e.g., CrossEntropyLoss for classification), and the optimizer will update the weights during training.\n",
    "5. **Fine-Tune the Model**: Finally, we can train the model. During training, the pre-trained layers (if not frozen) will also be updated based on the new task. You can train for a few epochs to fine-tune the model.\n",
    "6. **Evaluate the Model**: After training, it's important to evaluate the model on unseen data to check its performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: car...\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install opencv-python\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import data_loader as dl\n",
    "\n",
    "# Load your own dataset\n",
    "data_loader = dl.DataLoader()\n",
    "train_data, test_data = data_loader.get_dataset(\"car\")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Step 1:  Load a pre-trained model\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Step 2: Replace the Classifier Layers\n",
    "num_classes = 2\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes) \n",
    "\n",
    "# Step 3: Free the pre-trained (feature extraction) Layers\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only the classifier layer' weights updated\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/5, Loss: 0.719083571434021\n",
      "Epoch 2/5, Loss: 0.5921075284481049\n",
      "Epoch 3/5, Loss: 0.46992973387241366\n",
      "Epoch 4/5, Loss: 0.42425309419631957\n",
      "Epoch 5/5, Loss: 0.39163399040699004\n",
      "Finished fine-tuning\n",
      "Computation Time: 30.68341612815857 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #I am going to use GPU if it is available\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 5\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"Finished fine-tuning\")\n",
    "\n",
    "computation_time = time.time() - start_time\n",
    "print(f\"Computation Time: {computation_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.881%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        prob, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        y_pred.extend(predicted.cpu())  # Move predicted values back to CPU for further processing\n",
    "        y_true.extend(labels.cpu())    # Move true labels back to CPU for further processing\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:0.3f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[64  8]\n",
      " [ 9 87]]\n",
      "Accuracy of the model is: 89.88%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "y_pred_list = [item.item() for item in y_pred]  # Using .item() to get the scalar value from each tensor\n",
    "y_true_list = [item.item() for item in y_true]\n",
    "\n",
    "class_labels = pd.DataFrame({'Predicted': y_pred_list, 'Truth_Label': y_true_list})\n",
    "class_counts = class_labels['Truth_Label'].value_counts().sort_index()\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "TP = np.diag(cm)  \n",
    "FP = np.sum(cm, axis=0) - TP  \n",
    "FN = np.sum(cm, axis=1) - TP  \n",
    "\n",
    "result_df = pd.DataFrame({\"True Positive\": TP, \"False Positive\": FP, \"False Negative\": FN, \"Number_Samples\": class_counts.values})\n",
    "print(f\"Accuracy of the model is: {100*result_df['True Positive'].sum()/len(class_labels):0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task**: Fine-tuning and Evaluating Models\n",
    "\n",
    "#### **Objective**:\n",
    "Now that you have fine-tuned your models using your own training dataset, it's time to test your results. You will evaluate the performance of your fine-tuned models using **True Positives (TP)**, **False Positives (FP)**, **Accuracy**, and **Computation Time**.\n",
    "\n",
    "#### **Instructions**:\n",
    "1. **Try Different Methods**: \n",
    "   - Experiment with different methods for fine-tuning your models. You can try adjusting hyperparameters, using different optimizers, or adding regularization techniques like dropout.\n",
    "   - Test your models on the same **pet images** dataset to ensure consistent results.\n",
    "\n",
    "2. **Compute the Following Metrics**:\n",
    "   - **True Positives (TP)**: Correctly classified positive samples.\n",
    "   - **False Positives (FP)**: Incorrectly classified negative samples as positive.\n",
    "   - **Accuracy**: The overall accuracy of the model.\n",
    "   - **Computation Time**: Measure how long it takes for your model to process the images and make predictions.\n",
    "\n",
    "3. **Compare the Results**:\n",
    "   After evaluating all models, compile the results into a table. The table should have the following structure:\n",
    "\n",
    "| Model          | True Positives (TP) | False Positives (FP) | Accuracy (%) | Computation Time (s) |\n",
    "|----------------|---------------------|----------------------|--------------|----------------------|\n",
    "| ResNet50       | 500                 | 20                   | 95.0         | 1.2                  |\n",
    "| AlexNet        | 450                 | 50                   | 90.0         | 1.1                  |\n",
    "| VGG16          | 480                 | 40                   | 92.0         | 1.5                  |\n",
    "| DenseNet121    | 470                 | 30                   | 93.5         | 1.3                  |\n",
    "| MobileNetV2    | 490                 | 10                   | 96.0         | 0.8                  |\n",
    "\n",
    "#### **How to Measure Computation Time**:\n",
    "You can measure the computation time for inference using Python's `time` module. Hereâ€™s an example of how to do it:\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "# Start the timer before inference\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform inference (model prediction)\n",
    "do_your_job\n",
    "\n",
    "# Calculate the computation time\n",
    "computation_time = time.time() - start_time\n",
    "print(f\"Computation Time: {computation_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
