{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1**: Load the Dataset\n",
    "\n",
    "Load the MNIST dataset and inspect the data structure.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Load the dataset using the provided data loader.\n",
    "- Explore the dimensions of the training dataset.\n",
    "---\n",
    "\n",
    "#### **Step 2**: Preprocess the Data\n",
    "Prepare the data for training by normalizing pixel values and converting data into tensors.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Convert the dataset into PyTorch tensors.\n",
    "- Normalize the pixel values to a range of -1 to 1. (mean =0.5, std = 0.5)\n",
    "- Visualize a few sample images along with their labels.\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: mnist...\n",
      "MNIST dataset already downloaded\n",
      "<class 'torch.Tensor'>\n",
      "Image dimenstion: torch.Size([1, 28, 28])\n",
      "Number of images in the 1st batch set: 64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import data_loader as dl\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_loader = dl.DataLoader()\n",
    "mnist_trdata, mnist_testdata = data_loader.get_dataset(\"mnist\")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(mnist_trdata, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(mnist_testdata, batch_size=64, shuffle=True)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(type(images))\n",
    "print(f\"Image dimenstion: {images.shape[1:]}\")\n",
    "print(f\"Number of images in the 1st batch set: {labels.shape[0]}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))  \n",
    "for k in range(3):\n",
    "    axes[k].imshow(images[k].numpy().squeeze(), cmap='Greys_r')  \n",
    "    axes[k].set_title(f\"Image label is {labels[k]}\")\n",
    "    axes[k].axis('off')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3**: Build the Neural Network\n",
    "Define a neural network to classify the digits.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Design a network architecture with \n",
    "    - **Input Layer**: 784 input units (corresponding to flattened 28x28 pixel images).\n",
    "    - **First hidden layer** with 128 units.\n",
    "    - **Second hidden layer** with 64 units.\n",
    "    - **Output Layer**: - 10 units (corresponding to the 10 possible digit classes: 0–9).\n",
    "-  Use **ReLU** (Rectified Linear Unit) for the two hidden layers to introduce non-linearity.\n",
    "-  Use **Softmax** for the output layer to produce probabilities for each digit class.\n",
    "- Create the neural network structure based on the above specifications and Initialize the model.\n",
    "- Before training the model, test its forward pass using a single input image to verify that the network is functioning as expected.\n",
    "- Visualize the the output using **helper module**.\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from utils import view_helper\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "# Test an image before training\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "img = images[img_idx]\n",
    "view_helper.view_classify(img.view(1, 28, 28), ps)\n",
    "print(f\"The highest probablity {ps.topk(1).values.item()* 100:.2f}% at class: {ps.topk(1).indices.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4**: Define Loss Function and Optimizer\n",
    "Set up a suitable loss function and optimizer to train the model.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- **Use Cross-Entropy Loss** as the loss function.\n",
    "    - **Note**: `nn.CrossEntropyLoss()` is commonly used when the model outputs raw logits (i.e., before applying LogSoftmax). It internally applies Softmax and computes the negative log-likelihood loss.\n",
    "    - **`nn.NLLLoss()`** is used when the model outputs log-probabilities (i.e., after applying LogSoftmax). It expects log-probabilities as inputs.\n",
    "\n",
    "- **Select an optimizer**, such as SGD or Adam, and set the learning rate.\n",
    "    - Example:\n",
    "    ```python\n",
    "    criterion = nn.CrossEntropyLoss()  # or nn.NLLLoss() if using LogSoftmax in the model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5**: Train the Model\n",
    "Train the neural network on the training dataset over multiple epochs.\n",
    "\n",
    "**Task:**\n",
    "- Loop through the training data for a specified number of epochs.\n",
    "- In each epoch:\n",
    "    - Perform a forward pass to compute the predictions.\n",
    "    - Compute the loss for both training and testing.\n",
    "    - Backpropagate the error and update the model weights.\n",
    "    - Track the training/testing loss for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    tot_train_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "        tot_train_loss += loss.item()\n",
    "    else:\n",
    "        tot_test_loss = 0\n",
    "        test_correct = 0 \n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:   \n",
    "                images = images.view(images.shape[0], -1)         \n",
    "                ps = model(images)\n",
    "                loss = criterion(ps, labels)\n",
    "                tot_test_loss += loss.item()\n",
    "\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                test_correct += equals.sum().item()\n",
    "\n",
    "        train_loss = tot_train_loss / len(trainloader.dataset)\n",
    "        test_loss = tot_test_loss / len(testloader.dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6**: Visualize Results\n",
    "Visualize the model's performance using appropriate plots.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Plot the training/testing loss curve over epochs.\n",
    "- Display a sample image using **helper module** and print the actual and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    ps = model(img)\n",
    "\n",
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "view_helper.view_classify(img.view(1, 28, 28), ps)\n",
    "print(f\"The highest probablity: {ps.topk(1).values.item()*100:.2f}% at class: {ps.topk(1).indices.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7**: Evaluate the Model\n",
    "Assess the model's performance on the testing dataset.\n",
    "**Task:**\n",
    "- Perform predictions on the test dataset.\n",
    "- Generate a confusion matrix to analyze classification errors.\n",
    "- Calculate accuracy by comparing predicted labels with actual labels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "pred_class = []\n",
    "actual_class = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:   \n",
    "        images = images.view(images.shape[0], -1)         \n",
    "        ps = model(images)\n",
    "        loss = criterion(ps, labels)\n",
    "\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        pred_class.extend(top_class.numpy())\n",
    "        actual_class.extend(labels.numpy())\n",
    "\n",
    "pred_class = np.array(pred_class).flatten() \n",
    "actual_class = np.array(actual_class).flatten()               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "class_labels = pd.DataFrame({'Predicted': pred_class, 'Truth_Label': actual_class})\n",
    "class_counts = class_labels['Truth_Label'].value_counts().sort_index()\n",
    "cm = confusion_matrix(actual_class, pred_class)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "TP = np.diag(cm)  \n",
    "FP = np.sum(cm, axis=0) - TP  \n",
    "FN = np.sum(cm, axis=1) - TP  \n",
    "\n",
    "result_df = pd.DataFrame({\"True Positive\": TP, \"False Positive\": FP, \"False Negative\": FN, \"Number_Samples\": class_counts.values})\n",
    "print(f\"Accuracy of the model is: {result_df['True Positive'].sum()/len(class_labels):0.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 8**: Save the Model\n",
    "\n",
    "**Task:**\n",
    "- Save the trained model using PyTorch's `torch.save()` function.\n",
    "- Ensure the model state dictionary (**model.state_dict()**) is saved, as it contains the model parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'input_size': input_size,  \n",
    "    'output_size': output_size,  \n",
    "    'hidden_layers': hidden_sizes,  \n",
    "    'state_dict': model.state_dict()  \n",
    "}\n",
    "\n",
    "model_path = f\"./models/digit_classifier_model.pth\"\n",
    "torch.save(checkpoint, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
