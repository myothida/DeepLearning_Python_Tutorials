{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tutorial 16: Transformer (Custom)**\n",
    "\n",
    "In this tutorial, we will learn how to build and implement a custom Transformer model from scratch for a machine learning task. The Transformer model has revolutionized the field of natural language processing (NLP) and time series prediction by replacing traditional recurrent neural networks (RNNs) with a more efficient attention mechanism. This tutorial will guide you through the process of creating a Transformer model for sequence-to-sequence tasks, such as translation, text summarization, or time series forecasting.\n",
    "\n",
    "### **What You Will Learn**\n",
    "- Introduction to the Transformer architecture\n",
    "- Implementing the multi-head self-attention mechanism\n",
    "- Building the Encoder and Decoder components of the Transformer\n",
    "- Customizing the Transformer for specific tasks\n",
    "- Training the model on sample data\n",
    "- Evaluating and fine-tuning the model\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Understanding the Transformer Architecture**\n",
    "\n",
    "The Transformer architecture is composed of two main parts:\n",
    "- **Encoder**: This part processes the input sequence and generates feature representations that the decoder can use.\n",
    "- **Decoder**: This part generates the output sequence from the feature representations provided by the encoder.\n",
    "\n",
    "The key component that distinguishes Transformers from other architectures is the **self-attention mechanism**, which allows the model to focus on different parts of the input sequence as it generates an output.\n",
    "\n",
    "Here’s a simplified overview of the Transformer architecture:\n",
    "\n",
    "- **Multi-Head Attention**: A mechanism that allows the model to focus on different parts of the sequence simultaneously.\n",
    "- **Positional Encoding**: Since Transformers do not inherently process data in order (like RNNs), positional encodings are added to the input data to give it information about the position of each element in the sequence.\n",
    "- **Feed-Forward Networks**: After the attention layers, the output is passed through fully connected layers.\n",
    "- **Residual Connections and Layer Normalization**: Used to prevent vanishing gradients and to improve training stability.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Implementing the Transformer Components**\n",
    "\n",
    "Let’s now dive into the implementation of a custom Transformer model. We will start by implementing the essential components: the multi-head attention mechanism, the encoder, and the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Multi-Head Attention Layer**\n",
    "The multi-head attention mechanism allows the model to simultaneously focus on different parts of the sequence. Here's the code to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "    self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, tokens_num, embedding_dim = input.shape\n",
    "    Q = self.Q_weights(input) \n",
    "    K = self.K_weights(input) \n",
    "    V = self.V_weights(input)\n",
    "\n",
    "    attention_scores = Q @ K.transpose(1, 2)  \n",
    "    attention_scores = attention_scores.masked_fill(\n",
    "        self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "        -torch.inf\n",
    "    )\n",
    "    attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return attention_scores @ V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "    self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "    self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "    scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "\n",
    "    scores_change = self.linear(scores_change)\n",
    "    return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "        nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
    "        nn.Dropout(config[\"dropout_rate\"])\n",
    "    )\n",
    "\n",
    "\n",
    "    def forward(self, input):        \n",
    "        return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multi_head = MultiHeadAttention(config)\n",
    "    self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    self.feed_forward = TransformerEncoder(config)\n",
    "    self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    residual = input\n",
    "    x = self.multi_head(self.layer_norm_1(input))\n",
    "    x = x + residual\n",
    "\n",
    "    residual = x\n",
    "    x = self.feed_forward(self.layer_norm_2(x))\n",
    "    return x + residual\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
    "    self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
    "\n",
    "    blocks = [TransformerBlock(config) for _ in range(config[\"layers_num\"])]\n",
    "    self.layers = nn.Sequential(*blocks)\n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "    self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
    "\n",
    "  def forward(self, token_ids):\n",
    "    batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "    x = self.token_embedding_layer(token_ids)\n",
    "    sequence = torch.arange(tokens_num, device=device)\n",
    "    x = x + self.positional_embedding_layer(sequence)\n",
    "\n",
    "    x = self.layers(x)\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.unembedding(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt_ids, max_tokens):\n",
    "    output_ids = prompt_ids\n",
    "    for _ in range(max_tokens):\n",
    "      if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "        break\n",
    "      with torch.no_grad():\n",
    "        logits = model(output_ids)\n",
    "\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "      output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "      \n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "  \n",
    "  model.eval()\n",
    "  prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "  return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result before training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 118\n",
      "Ecoded Vector of 'Given a recent sample of ' is: tensor([10, 15, 82, 85, 65])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Given a recent sample of target willing order You are within the 1 loss profitability This use meeting up target **two percent must use use more'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.tokenizer import CharTokenizer, WordTokenizer\n",
    "\n",
    "text_paragraph = \"\"\"\n",
    "You are a manufacturer of hip implants. The doctor who will use your implants in surgeries has a requirement: he is willing to accept implants that are 1 mm bigger or smaller than the specified target size. This means the implant sizes must fall within a 2 mm range of the target size, i.e., ±1 mm from the target.\n",
    "Additionally, your financial officer has stated that in order to maintain profitability, you can afford to discard **1 out of every 1000 implants**. This means that the size distribution of your implants must be such that only 0.1% of implants fall outside the acceptable ±1 mm range.\n",
    "Given a recent sample of 1000 implants from the factory, the task is to evaluate whether the factory is meeting the specified quality and profitability requirements. If more than one percent of the implants fall outside the ±1 mm range, the factory will incur a loss due to excess waste.\n",
    "You are a manufacturer of surgical gloves. The doctor who will use your gloves in surgeries has a requirement: the gloves must fit tightly but comfortably. \n",
    "The doctor is willing to accept a slight variance of up to **2 cm** in the glove's length from the specified target size. This means the gloves must fall within a **4 cm range** of the target size, \n",
    "i.e., ±2 cm from the target. Additionally, your financial officer has stated that to maintain profitability, you can afford to discard **2 out of every 1000 gloves**. \n",
    "This means that the size distribution of your gloves must be such that only 0.2% of gloves fall outside the acceptable ±2 cm range. Given a recent sample of 1000 gloves from the factory, \n",
    "the task is to evaluate whether the factory is meeting the specified quality and profitability requirements. If more than **two percent** of \n",
    "the gloves fall outside the ±2 cm range, the factory will incur a loss due to excess waste.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = WordTokenizer.train_from_text(text_paragraph)\n",
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")\n",
    "print(f\"Ecoded Vector of 'Given a recent sample of ' is: {tokenizer.encode(\"Given a recent sample of \")}\")\n",
    "\n",
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 26, # Length of context window for each token\n",
    "  \"embedding_dim\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]\n",
    "\n",
    "model = DemoGPT(config).to(device)\n",
    "generate_with_prompt(model, tokenizer, \"Given a recent sample of\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset length: 295\n",
      "Testing Dataset length: 139\n",
      "Step 0. Loss 4.941\n",
      "Evaluation Step 0. Test Loss: 5.514\n",
      "Step 10. Loss 0.934\n",
      "Evaluation Step 10. Test Loss: 0.918\n",
      "Step 20. Loss 0.582\n",
      "Evaluation Step 20. Test Loss: 0.518\n",
      "Step 30. Loss 0.394\n",
      "Evaluation Step 30. Test Loss: 0.316\n",
      "Step 40. Loss 0.262\n",
      "Evaluation Step 40. Test Loss: 0.188\n",
      "Step 50. Loss 0.112\n",
      "Evaluation Step 50. Test Loss: 0.133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Given a recent sample of 1000 gloves from the factory, the task is to evaluate whether the factory is meeting the specified quality and profitability requirements.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This result is after training\n",
    "from utils.sequentialdataset import TokenIdsDataset\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "train_iterations = 60\n",
    "evaluation_interval = 10\n",
    "learning_rate=4e-4\n",
    "\n",
    "train_data = tokenizer.encode(text_paragraph).to(device)\n",
    "train_dataset = TokenIdsDataset(train_data, config[\"context_size\"])\n",
    "print(f\"Training Dataset length: {len(train_dataset)}\")\n",
    "\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset, num_samples=batch_size * train_iterations, replacement=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "test_paragraph = \"\"\"\n",
    "You are a manufacturer of surgical gloves. The doctor who will use your gloves in surgeries has a requirement: the gloves must fit tightly but comfortably. \n",
    "The doctor is willing to accept a slight variance of up to **2 cm** in the glove's length from the specified target size. This means the gloves must fall within a **4 cm range** of the target size, \n",
    "i.e., ±2 cm from the target. Additionally, your financial officer has stated that to maintain profitability, you can afford to discard **2 out of every 1000 gloves**. \n",
    "This means that the size distribution of your gloves must be such that only 0.2% of gloves fall outside the acceptable ±2 cm range. Given a recent sample of 1000 gloves from the factory, \n",
    "the task is to evaluate whether the factory is meeting the specified quality and profitability requirements. If more than **two percent** of \n",
    "the gloves fall outside the ±2 cm range, the factory will incur a loss due to excess waste.\n",
    "\"\"\"\n",
    "test_data = tokenizer.encode(test_paragraph).to(device)\n",
    "test_dataset = TokenIdsDataset(test_data, config[\"context_size\"])\n",
    "print(f\"Testing Dataset length: {len(test_dataset)}\")\n",
    "\n",
    "test_sampler = RandomSampler(test_dataset, num_samples=batch_size * train_iterations, replacement=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=test_sampler, drop_last = True)\n",
    "\n",
    "\n",
    "for step_num, sample in enumerate(train_dataloader):\n",
    "    model.train()\n",
    "    input, targets = sample\n",
    "    logits = model(input)\n",
    "\n",
    "    logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "    targets_view = targets.view(batch_size * config[\"context_size\"])\n",
    "\n",
    "    loss = F.cross_entropy(logits_view, targets_view)  \n",
    "    loss.backward()  \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True) #reduce memory usage\n",
    "\n",
    "    if step_num%evaluation_interval==0:\n",
    "        print(f\"Step {step_num}. Loss {loss.item():.3f}\")\n",
    "\n",
    "    # Evaluation on the test dataset at regular intervals\n",
    "    if step_num % evaluation_interval == 0:\n",
    "        model.eval()  # Switch model to evaluation mode\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient computation for testing\n",
    "            for test_sample in test_dataloader:\n",
    "                test_input, test_targets = test_sample\n",
    "                test_logits = model(test_input)\n",
    "                                \n",
    "                test_logits_view = test_logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "                test_targets_view = test_targets.view(batch_size * config[\"context_size\"])\n",
    "                \n",
    "                test_loss = F.cross_entropy(test_logits_view, test_targets_view)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "        average_test_loss = total_test_loss / len(test_dataloader)\n",
    "        print(f\"Evaluation Step {step_num}. Test Loss: {average_test_loss:.3f}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "generate_with_prompt(model, tokenizer, \"Given a recent sample of\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\09_Projects\\04_DeepLearning\\DeepLearning_Python_Tutorials\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Given a recent sample of all the people interviewed for this study reported a strong preference of women on all political topics.\\n\\nA third of the study\\'s authors had been appointed by members of Parliament.\\n\\nAs of this writing, only four men and two women were on the advisory board: Mr Turnbull, Mr Davis, Lisa Raitt, and Ms Abbott.\\n\\nMr Turnbull\\'s chief of staff, David Lakin, resigned at the end of February after a series of embarrassing controversies.\\n\\nThis past week it emerged that the ALP had tried to get the public to agree on issues such as the minimum wage and the abolition of the free childcare program.\\n\\nOpposition spokesman Paul Nuttall said yesterday he was \"shocked and saddened\" to hear about the report, calling it \"ridiculous\".\\n\\n\"While people believe everything this group says they care about, they are not really going to change their minds,\" he said.\\n\\n\"We are not changing our minds'}]\n"
     ]
    }
   ],
   "source": [
    "#Using the hugging face\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained model for text generation\n",
    "generator = pipeline(\"text-generation\", model = 'gpt2')\n",
    "result = generator(\"Given a recent sample of\", max_length=200, truncation=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
