{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tutorial 03: Building Single-Layer-NN\n",
    "PyTorch is a popular open-source machine learning framework that provides flexibility and ease of use for developing neural networks. In this tutorial, we'll walk through the steps to build, train, and evaluate a simple neural network for predicting car fule efficiency.\n",
    "\n",
    "##### Predicting Car Fuel Efficiency\n",
    "Assume that you want to predict the car fuel efficiency based on the **Engine size**, **Car Weight** and **Number of cylinders**. \n",
    "\n",
    "| Index | $ x_1 $ (Engine size in liters) | $ x_2 $ (Car weight in pounds) | $ x_3 $ (Number of cylinders) | $ y $ (Fuel efficiency in MPG) |\n",
    "|-------|------------------------------------|-----------------------------------|---------------------------------|----------------------------------|\n",
    "| 1     | 2.0                                | 3000                              | 4                               | 30                               |\n",
    "| 2     | 3.5                                | 4000                              | 6                               | 20                               |\n",
    "| 3     | 1.8                                | 2800                              | 4                               | 35                               |\n",
    "| 4     | 2.5                                | 3500                              | 6                               | 25                               |\n",
    "| 5     | 3.0                                | 3700                              | 6                               | 22                               |\n",
    "\n",
    "In the above problem, the input data point has 3 features, making the feature vector a $1 \\times 3$. Assuming that there is no hidden layer, the **single-layer neural network** will consist of an input layer with three neurons (one for each feature) and an output layer with one neuron (predicting the fuel efficiency in MPG). This simple network can be represented graphically as: \n",
    "\n",
    "<img src=\"imgs/slp.png\" width=600px>\n",
    "\n",
    "And Mathematically, \n",
    "\n",
    "$y = x \\cdot W + b$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x$ is the input feature vector ($1 \\times 3$).\n",
    "- $W$ is the weight matrix ($3 \\times 1$), representing the connections between input features and the output.\n",
    "- $b$ is the bias term ($1 \\times 1$).\n",
    "- $y$ is the predicted output ($1 \\times 1$), representing the fuel efficiency.\n",
    "\n",
    "We'll use **Mean Squared Error (MSE)** as the loss function to measure the difference between the predicted and actual values of fuel efficiency. The optimizer we choose will update the weights and biases during training to minimize this loss.\n",
    "\n",
    "---\n",
    "\n",
    "In this case, we do not need **an activation function** for the output layer of the neural network. Because: \n",
    "\n",
    "- The task is regression, where the goal is to predict a continuous value (fuel efficiency in MPG).\n",
    "- Activation functions like **ReLU**, **Sigmoid**, or **Tanh** are often used in hidden layers or in specific cases for output layers. However, for regression problems, the output should ideally **be a linear transformation of the inputs to preserve the continuous nature of the target variable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    \"x1\": [2.0, 3.5, 1.8, 2.5, 3.0],\n",
    "    \"x2\": [3000, 4000, 2800, 3500, 3700],\n",
    "    \"x3\": [4, 6, 4, 6, 6],\n",
    "    \"y\": [30, 20, 35, 25, 22]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, The **torch.nn** module is the building block for creating and training neural networks in PyTorch. It provides a variety of predefined layers, loss functions, and utilities for defining and working with neural networks.\n",
    "Usage in code:\n",
    "- Define neural network architectures using layers like nn.Linear (**fully connected layers**), nn.Conv2d (**convolutional layers**), etc.\n",
    "- Use loss functions like nn.CrossEntropyLoss or nn.MSELoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SingleLayerNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.fc = nn.Linear(3, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SingleLayerNN()\n",
    "print(model.fc.weight)\n",
    "print(model.fc.bias)\n",
    "\n",
    "####manual define the bias\n",
    "model.fc.weight.data.normal_(std=0.1)\n",
    "model.fc.bias.data.fill_(0)\n",
    "print(model.fc.bias, model.fc.weight)\n",
    "\n",
    "X = df[[\"x1\", \"x2\", \"x3\"]].values\n",
    "y_actual = df['y'].values \n",
    "X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "X_tensor = torch.tensor(X_normalized, dtype=torch.float32)  \n",
    "y_tensor = torch.tensor(y_actual, dtype=torch.float32).view(-1, 1)  \n",
    "\n",
    "y_pred = model.forward(X_tensor)\n",
    "print('Predicted Fuel Consumption:', y_pred.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted results you are seeing, such as [-0.0462392, -0.01373153, -0.02189323, 0.05566421, 0.02619975], are not meaningful because they are in a range that is far from the expected fuel efficiency values (e.g., 20, 25, 30 MPG). The reason for this is because we haven't trained the model yet. All weights are initialized **radonmly**. \n",
    "\n",
    "Below code trains the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  \n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    y_pred = model(X_tensor)\n",
    "    loss = criterion(y_pred, y_tensor)\n",
    "\n",
    "    # Zero gradients, backward pass, optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:  # Print every 200 epochs\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "y_pred = model.forward(X_tensor)\n",
    "print(\"Predicted Fuel Efficiency:\", y_pred.detach().numpy().flatten())\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    'Actual': y_actual,\n",
    "    'Predicted': y_pred.detach().numpy().flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_plot, x='Actual', y='Predicted')\n",
    "plt.plot([df_plot['Actual'].min(), df_plot['Actual'].max()],\n",
    "         [df_plot['Actual'].min(), df_plot['Actual'].max()],\n",
    "         color='red', linestyle='--', label='y_pred = y_actual')\n",
    "plt.title('Actual vs Predicted values after Optimization')\n",
    "plt.xlabel('Actual values (y)')\n",
    "plt.ylabel('Predicted values (y_pred)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Loss Function: `criterion = nn.MSELoss()`\n",
    "\n",
    "This line defines the **Mean Squared Error (MSE)** as the loss function. The MSE is used to measure how far the predicted outputs are from the actual values (the target).\n",
    "\n",
    "##### Mathematical Definition of MSE:\n",
    "\n",
    "Given the predicted values $\\hat{y}_i$ and the actual target values $y_i$ for each sample $i$, the Mean Squared Error is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of samples in the dataset.\n",
    "- $\\hat{y}_i$ is the predicted value for the $i$-th sample (output from the model).\n",
    "- $y_i$ is the actual value (target) for the $i$-th sample.\n",
    "\n",
    "The MSE gives the average squared difference between the predicted and actual values. The goal of training is to minimize this error (i.e., make the predictions as close as possible to the actual values).\n",
    "\n",
    "##### 2. Optimizer: `optimizer = optim.SGD(model.parameters(), lr=0.01)`\n",
    "\n",
    "This line defines the **Stochastic Gradient Descent (SGD)** optimizer, which is responsible for adjusting the model's weights and biases during training. The optimizer uses the gradient of the loss function with respect to the model's parameters to update them in the direction that minimizes the loss.\n",
    "\n",
    "###### Mathematical Explanation of SGD:\n",
    "\n",
    "Let $\\theta$ represent the parameters of the model (weights and biases). The goal is to minimize the loss function $\\mathcal{L}(\\theta)$, which in this case is the MSE loss. The optimizer updates the parameters in the opposite direction of the gradient of the loss function with respect to those parameters.\n",
    "\n",
    "The update rule for SGD is given by:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ are the model parameters (weights and biases).\n",
    "- $\\eta$ is the **learning rate** (here $\\eta = 0.01$).\n",
    "- $\\nabla_\\theta \\mathcal{L}(\\theta)$ is the gradient of the loss function $\\mathcal{L}(\\theta)$ with respect to the parameters $\\theta$.\n",
    "\n",
    "This means that at each step, the optimizer adjusts the parameters by a small amount in the direction that reduces the loss, determined by the gradient of the loss with respect to the parameters. The learning rate $\\eta$ controls how large the updates are — smaller values make the updates more gradual, while larger values can make the updates faster but risk overshooting the optimal solution.\n",
    "\n",
    "##### Summary:\n",
    "- **MSE Loss**: Measures the error between predicted values and actual values.\n",
    "- **SGD Optimizer**: Updates model parameters based on the gradient of the loss function, aiming to minimize the error. The learning rate controls the step size of these updates.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task: Create a Neural Network Model with One Hidden Layer\n",
    "\n",
    "In this task, you will create a neural network with one hidden layer consisting of 2 neurons and 1 output neuron. The model will predict car fuel efficiency based on the input features: **Engine size**, **Car weight**, and **Number of cylinders**.\n",
    "\n",
    "###### Steps:\n",
    "\n",
    "1. **Define the Model**: Create a simple neural network class using PyTorch with one hidden layer consisting of 2 neurons. Use the following architecture:\n",
    "   - **Input Layer**: 3 neurons (one for each input feature: Engine size, Car weight, Number of cylinders)\n",
    "   - **Hidden Layer**: 2 neurons\n",
    "   - **Output Layer**: 1 neuron (predicted fuel efficiency)\n",
    "\n",
    "2. **Choose the Activation Function**: Use the **ReLU** activation function for the hidden layer.\n",
    "\n",
    "3. **Set up the Loss Function and Optimizer**: Use Mean Squared Error (MSE) loss function and Stochastic Gradient Descent (SGD) optimizer.\n",
    "\n",
    "4. **Train the Model**: Train the model using the car fuel efficiency data, normalize the input features, and update the weights and biases using the optimizer.\n",
    "\n",
    "5. **Make Predictions**: After training, use the model to predict fuel efficiency for the given input features.\n",
    "\n",
    "<img src=\"imgs/nn_one_hidden_layer.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "\n",
    "##### What is an Activation Function?\n",
    "\n",
    "An activation function is a mathematical function applied to the output of each neuron in a neural network. It helps decide whether a neuron should be activated or not, based on the input. Without activation functions, the neural network would only be able to perform linear transformations, which limits its ability to learn complex patterns.\n",
    "\n",
    "##### The Role of Activation Functions:\n",
    "1. **Introduce Non-Linearity**: Most real-world data are non-linear. Activation functions help neural networks learn non-linear decision boundaries.\n",
    "2. **Determine Output**: The activation function determines the output of each neuron in the network based on the input received.\n",
    "\n",
    "#### Common Types of Activation Functions\n",
    "\n",
    "##### 1. **Sigmoid Activation Function**\n",
    "\n",
    "The **Sigmoid** activation function squashes input values into the range (0, 1). It is often used in binary classification problems.\n",
    "\n",
    "**Mathematical Formula**:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "Where:\n",
    "- $ x $ is the input to the neuron.\n",
    "- $ \\sigma(x) $ is the output of the sigmoid function.\n",
    "\n",
    "**Properties**:\n",
    "- Outputs values between 0 and 1.\n",
    "- It is used in the output layer of binary classifiers.\n",
    "- The sigmoid function is **differentiable**, which allows the model to be trained via backpropagation.\n",
    "\n",
    "**Example**:\n",
    "If $ x = 2 $, then:\n",
    "$$\n",
    "\\sigma(2) = \\frac{1}{1 + e^{-2}} \\approx 0.88\n",
    "$$\n",
    "\n",
    "##### 2. **Hyperbolic Tangent (Tanh) Activation Function**\n",
    "\n",
    "The **Tanh** (hyperbolic tangent) function is similar to the sigmoid but scales the output to the range (-1, 1). It is generally preferred over the sigmoid when working with hidden layers.\n",
    "\n",
    "**Mathematical Formula**:\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "Where:\n",
    "- $ x $ is the input to the neuron.\n",
    "- $ \\tanh(x) $ is the output of the tanh function.\n",
    "\n",
    "**Properties**:\n",
    "- Outputs values between -1 and 1.\n",
    "- The tanh function is **differentiable** and is used when we need outputs that can be both positive and negative.\n",
    "\n",
    "**Example**:\n",
    "If $ x = 1 $, then:\n",
    "$$\n",
    "\\tanh(1) = \\frac{e^1 - e^{-1}}{e^1 + e^{-1}} \\approx 0.76\n",
    "$$\n",
    "\n",
    "##### 3. **ReLU (Rectified Linear Unit) Activation Function**\n",
    "\n",
    "The **ReLU** activation function is one of the most commonly used activation functions in modern neural networks. It outputs the input directly if it is positive, otherwise, it outputs zero.\n",
    "\n",
    "**Mathematical Formula**:\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "Where:\n",
    "- $ x $ is the input to the neuron.\n",
    "- The output is either 0 if $ x $ is less than 0, or $ x $ itself if $ x $ is greater than 0.\n",
    "\n",
    "**Properties**:\n",
    "- It is computationally efficient.\n",
    "- Helps with the vanishing gradient problem by allowing gradients to flow for positive values.\n",
    "- However, it can cause dead neurons when inputs are negative (which i\n",
    "\n",
    "\n",
    "In practice, the **ReLU function is used almost exclusively as the activation function for hidden layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNWithHiddenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithHiddenLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 2)  \n",
    "        self.relu = nn.ReLU()       \n",
    "        self.fc2 = nn.Linear(2, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  \n",
    "        x = self.relu(x) \n",
    "        x = self.fc2(x)  \n",
    "        return x\n",
    "\n",
    "model = SimpleNNWithHiddenLayer()\n",
    "X = df[[\"x1\", \"x2\", \"x3\"]].values\n",
    "y_actual = df['y'].values \n",
    "X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "X_tensor = torch.tensor(X_normalized, dtype=torch.float32)  \n",
    "y_tensor = torch.tensor(y_actual, dtype=torch.float32).view(-1, 1)  \n",
    "\n",
    "y_pred = model.forward(X_tensor)\n",
    "print('Predicted Fuel Consumption:', y_pred.detach().numpy().flatten())\n",
    "\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  \n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    y_pred = model(X_tensor)\n",
    "    loss = criterion(y_pred, y_tensor)\n",
    "\n",
    "    # Zero gradients, backward pass, optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:  # Print every 200 epochs\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "y_pred = model.forward(X_tensor)\n",
    "print(\"Predicted Fuel Efficiency:\", y_pred.detach().numpy().flatten())\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    'Actual': y_actual,\n",
    "    'Predicted': y_pred.detach().numpy().flatten()\n",
    "})\n",
    "\n",
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)\n",
    "print(model.fc2.weight)\n",
    "print(model.fc2.bias)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_plot, x='Actual', y='Predicted')\n",
    "plt.plot([df_plot['Actual'].min(), df_plot['Actual'].max()],\n",
    "         [df_plot['Actual'].min(), df_plot['Actual'].max()],\n",
    "         color='red', linestyle='--', label='y_pred = y_actual')\n",
    "plt.title('Actual vs Predicted values after Optimization')\n",
    "plt.xlabel('Actual values (y)')\n",
    "plt.ylabel('Predicted values (y_pred)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Does adding a hidden layer improve the performance of the model in predicting fuel consumption, and why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of creating your own class, you can use `torch.nn.Sequential` to build a simple neural network model. `torch.nn.Sequential` allows you to define the layers of the model in a linear fashion without needing to manually define a forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 2),  \n",
    "    nn.ReLU(),        \n",
    "    nn.Linear(2, 1)   \n",
    ")\n",
    "\n",
    "X = df[[\"x1\", \"x2\", \"x3\"]].values\n",
    "y_actual = df['y'].values\n",
    "X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "X_tensor = torch.tensor(X_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_actual, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "y_pred = model(X_tensor)\n",
    "print(y_pred)\n",
    "\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  \n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    y_pred = model(X_tensor)\n",
    "    loss = criterion(y_pred, y_tensor)\n",
    "\n",
    "    # Zero gradients, backward pass, optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "y_pred = model.forward(X_tensor)\n",
    "print(\"Predicted Fuel Efficiency:\", y_pred.detach().numpy().flatten())\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    'Actual': y_actual,\n",
    "    'Predicted': y_pred.detach().numpy().flatten()\n",
    "})\n",
    "\n",
    "# Hidden Lyaer: weights and biases\n",
    "fc1_weights = model[0].weight.data.detach().numpy()\n",
    "fc1_bias = model[0].bias.data.detach().numpy() \n",
    "\n",
    "# Output Layer: weights and biases\n",
    "fc2_weights = model[2].weight.data.detach().numpy()  \n",
    "fc2_bias = model[2].bias.data.detach().numpy()      \n",
    "\n",
    "\n",
    "print(\"First Layer Weights (fc1):\")\n",
    "print(fc1_weights)\n",
    "print(\"First Layer Biases (fc1):\")\n",
    "print(fc1_bias)\n",
    "\n",
    "print(\"Second Layer Weights (fc2):\")\n",
    "print(fc2_weights)\n",
    "print(\"Second Layer Biases (fc2):\")\n",
    "print(fc2_bias)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
