{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tutorial 10: Tokenizer**\n",
    "\n",
    "This tutorial explains the concept of a tokenizer and demonstrates how to implement and use a basic character-level tokenizer in Python. A tokenizer is a tool that converts text into tokens (smaller units, such as words or characters) and, in some cases, converts those tokens back into text.\n",
    "\n",
    "In this tutorial, we build \n",
    "- 1). A character-level tokenizer that works at the character granularity, encoding each character as a unique token ID and decoding token IDs back to text.\n",
    "- 2). A word-level tokenizer that works at the word granularity, encoding each word as a unique token ID and decoding token IDs back to text.\n",
    "- 3). An n-grams tokenizer that generates sequences of tokens based on an n-grams approach, a fundamental concept in language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_extraction_successful(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully extracted.\n",
      "Sample of Character-level 3-grams: tensor([ 5, 32,  1])\n",
      "Sample of Word-level 2-grams: tensor([ 0, 12])\n"
     ]
    }
   ],
   "source": [
    "from utils import text_helper\n",
    "from utils.tokenizer import CharTokenizer, WordTokenizer, NGramsTokenizer\n",
    "\n",
    "url = \"https://medium.com/letters-to-my-younger-self-embracing-emotions-and/bridging-theory-and-practice-1b277456400d\"  \n",
    "text = text_helper.extract_medium_post_content(url)\n",
    "if not is_extraction_successful(text):\n",
    "    print(\"Failed to extract valid content from the URL.\")\n",
    "else:\n",
    "    print(\"Content successfully extracted.\")\n",
    "\n",
    "char_tokenizer = CharTokenizer.train_from_text(text)\n",
    "encoded = char_tokenizer.encode(text)\n",
    "\n",
    "ngrams_tokenizer = NGramsTokenizer(n=3)\n",
    "ngrams = ngrams_tokenizer.generate_ngrams(encoded)\n",
    "print(\"Sample of Character-level 3-grams:\", ngrams[0])\n",
    "\n",
    "\n",
    "word_tokenizer = WordTokenizer.train_from_text(text)\n",
    "encoded = word_tokenizer.encode(text)\n",
    "\n",
    "ngrams_tokenizer = NGramsTokenizer(n=2)\n",
    "ngrams = ngrams_tokenizer.generate_ngrams(encoded)\n",
    "print(\"Sample of Word-level 2-grams:\", ngrams[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sample: [ 5 32  1 14 27  1 18 17 34 16 14 33 28 31  2  1  8  1 28 19 33 18 27  1]\n",
      "output sample: [21 18 14 31  1 32 33 34 17 18 27 33]\n",
      "Input tensor: tensor([18, 26, 32,  1, 39,  1, 16, 14, 27,  1, 18, 14, 32, 22, 25, 38,  1, 15,\n",
      "        18,  1, 25, 18, 14, 31]) and decoded text is 'ems â€” can easily be lear'\n",
      "Output tensor: tensor([27, 18, 17,  1, 33, 21, 31, 28, 34, 20, 21,  1]) and decoded text is 'ned through '\n",
      "------------------------------\n",
      "Using word level tokenizer\n",
      "input sample: [ 0 12 36  4 71]\n",
      "output sample: [ 55 103  44  51 123]\n",
      "Input tensor: tensor([38, 64, 56, 45, 10]) and decoded text is 'equations makes her feel a'\n",
      "Output tensor: tensor([ 96,  70,  59, 105,  78]) and decoded text is 'sense of intellectual superiority over'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from utils import sequentialdataset as sd\n",
    "import numpy as np\n",
    "\n",
    "#using char-level tokenizer\n",
    "tokenized_text = char_tokenizer.encode(text) \n",
    "dataset = sd.SequentialDataset(tokenized_text, seq_len=24, label_len=12)\n",
    "seq_x, seq_y = dataset[0] \n",
    "print(f\"input sample: {np.round(seq_x.detach().flatten().tolist(),2)}\")\n",
    "print(f\"output sample: {np.round(seq_y.detach().flatten().tolist(),2)}\")\n",
    "\n",
    "\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "print(f\"Input tensor: {x[0]} and decoded text is '{char_tokenizer.decode(x[0])}'\")\n",
    "print(f\"Output tensor: {y[0]} and decoded text is '{char_tokenizer.decode(y[0])}'\")\n",
    "\n",
    "\n",
    "## Using word level tokenizer\n",
    "print(\"------------------------------\")\n",
    "print(\"Using word level tokenizer\")\n",
    "tokenized_text = word_tokenizer.encode(text) \n",
    "dataset = sd.SequentialDataset(tokenized_text, seq_len=5, label_len=5)\n",
    "seq_x, seq_y = dataset[0] \n",
    "print(f\"input sample: {np.round(seq_x.detach().flatten().tolist(),2)}\")\n",
    "print(f\"output sample: {np.round(seq_y.detach().flatten().tolist(),2)}\")\n",
    "\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)\n",
    "x, y = next(iter(dataloader))\n",
    "print(f\"Input tensor: {x[0]} and decoded text is '{word_tokenizer.decode(x[0])}'\")\n",
    "print(f\"Output tensor: {y[0]} and decoded text is '{word_tokenizer.decode(y[0])}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
