{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tutorial 10: Tokenizer**\n",
    "\n",
    "This tutorial explains the concept of a tokenizer and demonstrates how to implement and use a basic character-level tokenizer in Python. A tokenizer is a tool that converts text into tokens (smaller units, such as words or characters) and, in some cases, converts those tokens back into text.\n",
    "\n",
    "In this tutorial, we build \n",
    "- 1). A character-level tokenizer that works at the character granularity, encoding each character as a unique token ID and decoding token IDs back to text.\n",
    "- 2). A word-level tokenizer that works at the word granularity, encoding each word as a unique token ID and decoding token IDs back to text.\n",
    "- 3). An n-grams tokenizer that generates sequences of tokens based on an n-grams approach, a fundamental concept in language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_extraction_successful(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import text_helper\n",
    "from utils.tokenizer import CharTokenizer, WordTokenizer, NGramsTokenizer\n",
    "\n",
    "url = \"https://medium.com/letters-to-my-younger-self-embracing-emotions-and/bridging-theory-and-practice-1b277456400d\"  \n",
    "text = text_helper.extract_medium_post_content(url)\n",
    "if not is_extraction_successful(text):\n",
    "    print(\"Failed to extract valid content from the URL.\")\n",
    "else:\n",
    "    print(\"Content successfully extracted.\")\n",
    "\n",
    "char_tokenizer = CharTokenizer.train_from_text(text)\n",
    "encoded = char_tokenizer.encode(text)\n",
    "\n",
    "ngrams_tokenizer = NGramsTokenizer(n=3)\n",
    "ngrams = ngrams_tokenizer.generate_ngrams(encoded)\n",
    "print(\"Sample of Character-level 3-grams:\", ngrams[0])\n",
    "\n",
    "\n",
    "word_tokenizer = WordTokenizer.train_from_text(text)\n",
    "encoded = word_tokenizer.encode(text)\n",
    "\n",
    "ngrams_tokenizer = NGramsTokenizer(n=2)\n",
    "ngrams = ngrams_tokenizer.generate_ngrams(encoded)\n",
    "print(\"Sample of Word-level 2-grams:\", ngrams[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from utils import sequentialdataset as sd\n",
    "import numpy as np\n",
    "\n",
    "#using char-level tokenizer\n",
    "tokenized_text = char_tokenizer.encode(text) \n",
    "dataset = sd.SequentialDataset(tokenized_text, seq_len=24, label_len=12)\n",
    "seq_x, seq_y = dataset[0] \n",
    "print(f\"input sample: {np.round(seq_x.detach().flatten().tolist(),2)}\")\n",
    "print(f\"output sample: {np.round(seq_y.detach().flatten().tolist(),2)}\")\n",
    "\n",
    "\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "print(f\"Input tensor: {x[0]} and decoded text is '{char_tokenizer.decode(x[0])}'\")\n",
    "print(f\"Output tensor: {y[0]} and decoded text is '{char_tokenizer.decode(y[0])}'\")\n",
    "\n",
    "\n",
    "## Using word level tokenizer\n",
    "print(\"------------------------------\")\n",
    "print(\"Using word level tokenizer\")\n",
    "tokenized_text = word_tokenizer.encode(text) \n",
    "dataset = sd.SequentialDataset(tokenized_text, seq_len=5, label_len=5)\n",
    "seq_x, seq_y = dataset[0] \n",
    "print(f\"input sample: {np.round(seq_x.detach().flatten().tolist(),2)}\")\n",
    "print(f\"output sample: {np.round(seq_y.detach().flatten().tolist(),2)}\")\n",
    "\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)\n",
    "x, y = next(iter(dataloader))\n",
    "print(f\"Input tensor: {x[0]} and decoded text is '{word_tokenizer.decode(x[0])}'\")\n",
    "print(f\"Output tensor: {y[0]} and decoded text is '{word_tokenizer.decode(y[0])}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
