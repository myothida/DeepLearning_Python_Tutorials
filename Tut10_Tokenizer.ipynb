{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tutorial 10: Tokenizer**\n",
    "\n",
    "This tutorial explains the concept of a tokenizer and demonstrates how to implement and use a basic character-level tokenizer in Python. A tokenizer is a tool that converts text into tokens (smaller units, such as words or characters) and, in some cases, converts those tokens back into text.\n",
    "\n",
    "In this tutorial, we build \n",
    "- 1). A character-level tokenizer that works at the character granularity, encoding each character as a unique token ID and decoding token IDs back to text.\n",
    "- 2). A word-level tokenizer that works at the word granularity, encoding each word as a unique token ID and decoding token IDs back to text.\n",
    "- 3). An n-grams tokenizer that generates sequences of tokens based on an n-grams approach, a fundamental concept in language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_extraction_successful(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully extracted.\n",
      "Sample of Character-level 3-grams: tensor([ 5, 32,  1])\n",
      "Sample of Word-level 2-grams: tensor([ 0, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0,  12,  36,   4,  71,  55, 103,  44,  51, 123, 119,  31,  95, 113,\n",
       "        101, 117,  28, 111,  27, 123,  90,  15,   5,  39,  58,  80,  23,  45,\n",
       "         79,  13,  34,  50, 110,  30,  57,   6, 110,  77,  52,   4,  54,  67,\n",
       "        103, 122,  37, 125, 116, 110,  65,  49,  41,  85, 114,  78,  83,  25,\n",
       "        107,   9,   4,  17,  73, 104, 102,  11,  56,  91,  56,  81, 120,  60,\n",
       "          7,  43, 108,  77,  97, 127, 104,  16, 125,  72,  90,  89,  76, 126,\n",
       "         24, 117, 100,  83,  87, 127,  23,  35,  18,  63, 116,  74,  32,   3,\n",
       "         65,  38,  64,  56,  45,  10,  96,  70,  59, 105,  78, 115,  92,  86,\n",
       "         72,  74,  94,   4,  71,  54, 117,  93, 103, 109,  33,  13,  83,  98,\n",
       "         69,  68,  42,  22,  26,   8, 110, 128,  20, 110,  66,  61,  40, 127,\n",
       "         69,  46,  59, 106,  22, 117,  21,  10,  99,  48,   2, 114,  47,  19,\n",
       "        118,  82, 121,  62,  29, 117,  83,  14, 108, 100,  90,  88,   1, 103,\n",
       "        122,  45,  34,  50, 112,  13,  84,  75,  53, 124])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import text_helper\n",
    "from utils.tokenizer import CharTokenizer, WordTokenizer, NGramsTokenizer\n",
    "\n",
    "url = \"https://medium.com/letters-to-my-younger-self-embracing-emotions-and/bridging-theory-and-practice-1b277456400d\"  \n",
    "text = text_helper.extract_medium_post_content(url)\n",
    "if not is_extraction_successful(text):\n",
    "    print(\"Failed to extract valid content from the URL.\")\n",
    "else:\n",
    "    print(\"Content successfully extracted.\")\n",
    "\n",
    "char_tokenizer = CharTokenizer.train_from_text(text)\n",
    "encoded = char_tokenizer.encode(text)\n",
    "\n",
    "ngrams_tokenizer = NGramsTokenizer(n=3)\n",
    "ngrams = ngrams_tokenizer.generate_ngrams(encoded)\n",
    "print(\"Sample of Character-level 3-grams:\", ngrams[0])\n",
    "\n",
    "\n",
    "word_tokenizer = WordTokenizer.train_from_text(text)\n",
    "encoded = word_tokenizer.encode(text)\n",
    "\n",
    "ngrams_tokenizer = NGramsTokenizer(n=2)\n",
    "ngrams = ngrams_tokenizer.generate_ngrams(encoded)\n",
    "\n",
    "print(\"Sample of Word-level 2-grams:\", ngrams[0])\n",
    "\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sample: [ 5 32  1 14 27  1 18 17 34 16 14 33 28 31  2  1  8  1 28 19 33 18 27  1]\n",
      "output sample: [21 18 14 31  1 32 33 34 17 18 27 33]\n",
      "Input tensor: tensor([33, 21, 14, 33, 33, 21, 18, 28, 31, 18, 33, 22, 16, 14, 25,  1, 17, 18,\n",
      "        29, 33, 21,  1, 14, 27]) and decoded text is 'thattheoretical depth an'\n",
      "Output tensor: tensor([17,  1, 29, 31, 14, 16, 33, 22, 16, 14, 25,  1]) and decoded text is 'd practical '\n",
      "------------------------------\n",
      "Using word level tokenizer\n",
      "input sample: [ 0 12 36  4 71]\n",
      "output sample: [ 55 103  44  51 123]\n",
      "Input tensor: tensor([ 96,  70,  59, 105,  78]) and decoded text is 'sense of intellectual superiority over'\n",
      "Output tensor: tensor([115,  92,  86,  72,  74]) and decoded text is 'those relying primarily on online'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from utils import sequentialdataset as sd\n",
    "import numpy as np\n",
    "\n",
    "#using char-level tokenizer\n",
    "tokenized_text = char_tokenizer.encode(text) \n",
    "dataset = sd.SequentialDataset(tokenized_text, seq_len=24, label_len=12)\n",
    "seq_x, seq_y = dataset[0] \n",
    "print(f\"input sample: {np.round(seq_x.detach().flatten().tolist(),2)}\")\n",
    "print(f\"output sample: {np.round(seq_y.detach().flatten().tolist(),2)}\")\n",
    "\n",
    "\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "print(f\"Input tensor: {x[0]} and decoded text is '{char_tokenizer.decode(x[0])}'\")\n",
    "print(f\"Output tensor: {y[0]} and decoded text is '{char_tokenizer.decode(y[0])}'\")\n",
    "\n",
    "\n",
    "## Using word level tokenizer\n",
    "print(\"------------------------------\")\n",
    "print(\"Using word level tokenizer\")\n",
    "tokenized_text = word_tokenizer.encode(text) \n",
    "dataset = sd.SequentialDataset(tokenized_text, seq_len=5, label_len=5)\n",
    "seq_x, seq_y = dataset[0] \n",
    "print(f\"input sample: {np.round(seq_x.detach().flatten().tolist(),2)}\")\n",
    "print(f\"output sample: {np.round(seq_y.detach().flatten().tolist(),2)}\")\n",
    "\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)\n",
    "x, y = next(iter(dataloader))\n",
    "print(f\"Input tensor: {x[0]} and decoded text is '{word_tokenizer.decode(x[0])}'\")\n",
    "print(f\"Output tensor: {y[0]} and decoded text is '{word_tokenizer.decode(y[0])}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
